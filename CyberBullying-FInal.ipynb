{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Detection App\n",
    "\n",
    "\n",
    "Bullying has shifted from school recess encounters to the boundless frontiers of the digital world. Victims can be targeted whenever and wherever they are. Bullies on the other hand, can hide behind their computer screens and not have to confront their victims face-to-face: they can also hide behind false avatars and fake user names. \n",
    "\n",
    "The Nemours Foundation reports that cyberbullying's effects are always negative, and some cyberbullying victims experience catastrophic effects such as self-harm, depression and anxiety, and even suicide.\n",
    "\n",
    "The famous microblogging platform Twitter has become a daily essential by having over 330 million monthly active users tweeting 5,787 messages every second (source). However, each passing day the platform is becoming a bully playground as hurtful tweets target individuals, especially teens in their fragile growing years. It is estimated that around 15,000 bullying-related tweets are posted every day (source). Furthermore another research suggests (source)\n",
    "\n",
    "This app was created as an attempt tackle the cyberbullying phenomenon on Twitter: the target is to develop a tool that can identify tweets that contain any bullying content and to flag their occurrence. The tool can scale up to trigger events such as various immediate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import operator\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "from math import log,isinf\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import nltk\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step would be to fetch Twitter data in real-time to then be able to analyze it and obtain the results we are looking for. We will use two different tools:\n",
    "\n",
    "‚Ä¢ Scraper: the tool will read the data from Twitter in real- time.\n",
    "\n",
    "‚Ä¢ Analyzer: this tool will listen to the Tweets obtained from the Scraper, preform the analysis and deliver  results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    conf = SparkConf().setMaster(\"local[2]\").setAppName(\"Streamer\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Creating a streaming context with batch interval of 10 sec\n",
    "    ssc = StreamingContext(sc, 10)\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    filename = 'datasets/new_data.csv'\n",
    "    target_class = '1'\n",
    "    \n",
    "    # Load the data\n",
    "    [train_data, dev_data, test_data] = load_data_from_csv(filename)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = train_classifier(train_data, target_class)\n",
    "    \n",
    "    # Evaluate the classifier   \n",
    "    in_class = []\n",
    "    not_in_class = []\n",
    "    \n",
    "    for datum in dev_data:\n",
    "        if classify(classifier, datum):\n",
    "            in_class.append(datum)\n",
    "        else:\n",
    "            not_in_class.append(datum)\n",
    "    evaluate(target_class, in_class, not_in_class)\n",
    "    \n",
    "    filename = 'finalized_model.sav'\n",
    "    pickle.dump(classifier, open(filename, 'wb'))\n",
    "    \n",
    "    stream2(ssc, 30)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scraper\n",
    "The scraper runs 3 tasks:\n",
    "\n",
    "‚Ä¢ Opens a socket\n",
    "\n",
    "‚Ä¢ Connects to the Twitter Streaming API\n",
    "\n",
    "‚Ä¢ Reads stream tweets from the Twitter streaming API\n",
    "\n",
    "‚Ä¢ Publishes/sends tweets to the port/socket in JSON format\n",
    "\n",
    "In order to read Tweets, we first need to set variables that contain the user credentials to access the Twitter API.\n",
    "We then specify a port/socket (that works as a channel) to publish the tweets to so that the Analyzer can then read streaming data from this same socket. We use an arbitrary port, bind a socket and local host and start streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream2(ssc, duration):\n",
    "\n",
    "    dataStream = ssc.socketTextStream(\"localhost\",9092)\n",
    "    dataStream.window(60)\n",
    "\n",
    "    tweets = dataStream.flatMap(lambda line:line.split('\\n'))\n",
    "    print('-----------------------------------------')\n",
    "    print('Start of Twitter Feed')\n",
    "    print('-----------------------------------------')\n",
    "    tweets.foreachRDD(rdd_classify)\n",
    "     \n",
    "    # Start the computation\n",
    "    ssc.start()\n",
    "    \n",
    "    ssc.awaitTerminationOrTimeout(duration)\n",
    "    ssc.stop(stopGraceFully = True)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Analyzer\n",
    "The Analyzer uses Spark to:\n",
    "\n",
    "‚Ä¢ Bind to the specified socket\n",
    "\n",
    "‚Ä¢ Listen to the tweets published by the scraper by reading data sent to the socket\n",
    "\n",
    "‚Ä¢ Preprocess the Tweets\n",
    "\n",
    "‚Ä¢ Count ‚ÄúBullying‚Äù versus ‚ÄúNot Bullying‚Äù tweets.\n",
    "\n",
    "‚Ä¢ Push the results for visualization\n",
    "\n",
    "We define the following ‚ÄúAnalyzer‚Äù parameters:\n",
    "‚Ä¢ The batch Interval to specify how frequently (in seconds) we want to update the streaming data, as well as the window duration.\n",
    "\n",
    "‚Ä¢ The SparkContext object that needs to be instantiated since it is the main entry point for Spark functionality, and the StreamingContext. Spark Streaming is an extension of the core Spark API that enables stream processing of live data streams. Given that we will perform some RDD transformations, we also start checkpoints that save the generated RDDs to a reliable storage.\n",
    "\n",
    "‚Ä¢ A DStream that will connect to a hostname and a port (the same as the one we had defined on the Scraper file). We use the ‚Äúwindow‚Äù to apply transformations over a sliding window of data (windows length and sliding window defined in the initial parameters).\n",
    "\n",
    "The classifier model is currently a part of the Analyzer application, and it trains on a set of labeled text statements (bullying vs. non-bullying).\n",
    "\n",
    "The Analyzer then prints out all tweets, displaying either ‚ÄúBully Alert!\" below offending tweets or \"Nothing to see here\", below regular harmless tweets.\n",
    "\n",
    "Ideally, a count function applying reduceByKey would sum the number of ‚Äúbullying‚Äù tweets per batch, window or overall. Specific thresholds could be set to trigger various events and a plot could be generated to show the number and trend of offending tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdd_classify(rdd):\n",
    "   \n",
    "    classifier = pickle.load(open('finalized_model.sav', 'rb'))  \n",
    "    file = open('datasets/badwords.txt', 'r')\n",
    "    read_file = file.read()\n",
    "    badwords = nltk.Text(nltk.word_tokenize(read_file))\n",
    "   \n",
    "\n",
    "    for tweet in rdd.collect():       \n",
    "        if tweet != '':\n",
    "            \n",
    "            tweet_words = nltk.Text(nltk.word_tokenize(tweet))\n",
    "            if classify(classifier, Datum(tweet, '1')):\n",
    "                if any(i in tweet_words for i in badwords.tokens) == True:\n",
    "                    print(tweet)\n",
    "                    print(\"Bully Alert!\")\n",
    "                else:\n",
    "                    print(tweet)\n",
    "                    print(\"Nothing to see here\")\n",
    "            else:\n",
    "                print(tweet)\n",
    "                print(\"Nothing to see here\")\n",
    "            print('-----------------------------------------')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(datum):\n",
    "    features = []\n",
    "    last_word = '^'\n",
    "    for word in lower(datum):\n",
    "        features.append(word)\n",
    "        features.append(last_word + \"_\" + word)\n",
    "        last_word = word\n",
    "    return set(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(data, class_of_interest):\n",
    "    \n",
    "    total_counts = Counter()\n",
    "    for datum in data:\n",
    "        \n",
    "        if datum.answer() == class_of_interest:\n",
    "            total_counts[True] += 1\n",
    "        else:\n",
    "            total_counts[False] += 1\n",
    "    \n",
    "    # 2. Compute p(c)\n",
    "    # The probability of each label. This should mirror total_counts.\n",
    "    total_probs = Counter()\n",
    "    total_probs[True] = total_counts[True] / (total_counts[True] + total_counts[False])\n",
    "    total_probs[False] = total_counts[False] / (total_counts[True] + total_counts[False])\n",
    "    \n",
    "    # 3. Collect count(f | c)\n",
    "    true_counts = Counter()\n",
    "    false_counts = Counter()\n",
    "   \n",
    "    # For each tweet in our dataset...\n",
    "    \n",
    "    for datum in data:\n",
    "        features = featurize(datum)\n",
    "        for feature in features:\n",
    "            if datum.answer() == class_of_interest:\n",
    "                true_counts[feature] += 1\n",
    "            else:\n",
    "                false_counts[feature] += 1\n",
    "  \n",
    "    # Add an UNK count\n",
    "    true_counts['__UNK__'] = 1\n",
    "    false_counts['__UNK__'] = 1\n",
    "    \n",
    "    # Smooth the counts (add 0.1 fake counts to each feature)\n",
    "    features = set(true_counts + false_counts)\n",
    "    \n",
    "    for feature in features:\n",
    "        true_counts[feature] += 0.1\n",
    "        false_counts[feature] += 0.1\n",
    "        \n",
    "    # 4. Compute p(f | c)\n",
    "    true_probs = Counter()\n",
    "    false_probs = Counter()\n",
    "    # p(f | c) = count(f, c) / count(c)\n",
    "    \n",
    "    for feature in true_counts:\n",
    "        true_probs[feature] = true_counts[feature] / total_counts[True];\n",
    "    for feature in false_counts:\n",
    "        false_probs[feature] = false_counts[feature] / total_counts[False];\n",
    "    #print(total_probs)\n",
    "    # 5. Return the model\n",
    "    return [total_probs, true_probs, false_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, datum):\n",
    "    # Unpack the model\n",
    "    [total_probs, true_probs, false_probs] = model\n",
    "    # Start the log scores at 0.0\n",
    "    true_score = 0.0\n",
    "    false_score = 0.0\n",
    "\n",
    "    # Featurize the input\n",
    "    features = featurize(datum)\n",
    "    \n",
    "    # Multiply in p(c)\n",
    "    true_score += log(total_probs[True])\n",
    "    false_score += log(total_probs[False])\n",
    "    \n",
    "    # Multiply in p(f | c) for each f\n",
    "    for feature in features:\n",
    "        if feature in true_probs:\n",
    "            true_score += log(true_probs[feature])\n",
    "        else:\n",
    "            true_score += log(true_probs['__UNK__'])\n",
    "        if feature in false_probs:\n",
    "            false_score += log(false_probs[feature])\n",
    "        else:\n",
    "            false_score += log(false_probs['__UNK__'])\n",
    "            \n",
    "    # Some error checking\n",
    "    if isinf(true_score) or isinf(false_score):\n",
    "        print (\"WARNING: either true_score or false_score is infinite\")\n",
    "    \n",
    "    # Return the most likely class.\n",
    "    #print(true_score > false_score)\n",
    "    return true_score > false_score\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The actual tweets stream is displayed within a specific window. In another window, once a tweet is classified as having bullying content following the two-step classification process, a ‚Äúbullying alert‚Äù message is posted right below the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  50.5\n",
      "Precision: 31.7\n",
      "Recall:    65.1\n",
      "-----------------------------------------\n",
      "Start of Twitter Feed\n",
      "-----------------------------------------\n",
      "The pocket of my jacket accommodates my Leuchtturm1917 A5 notebook brilliantly. https://t.co/6DLyzHuUKF\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Today:\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Fashion blitz...\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Enjoy!\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "#TrendyTuesday #HighLevel\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "@BaddieTwinz @slaytvnow @HIMpodcast Right here sis.. üòò https://t.co/jWZtpKSaXP\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "@HandsomeAssh0le I be telling my own brain stfu cus of that shit lmaoooo\n",
      "Bully Alert!\n",
      "-----------------------------------------\n",
      "Working in the city and having so many fucking Starbucks around really has me fucked up. #GOODMORNING üåé\n",
      "Bully Alert!\n",
      "-----------------------------------------\n",
      "Mood for Saturday!!!!! https://t.co/kz01aySNKu\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "listen, why wasn‚Äôt there a princess diaries 3?\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Just posted a photo @ Madison Square Park https://t.co/5xvUX6TMjO\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Which show Funnier?\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "My haters either on they way to work or they arrived\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Never thought I would miss the NJ Motor Vehicle Department.\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "@traplord130 luv u sm enjoy your day superstar ‚≠êÔ∏è‚≠êÔ∏è\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Survive together hmm I wonder xbox live plus switch network and yet you white knighted the crap out of cross play a‚Ä¶ https://t.co/HvmSv16Ske\n",
      "Bully Alert!\n",
      "-----------------------------------------\n",
      "Who could forget. https://t.co/IPv45J2iSW\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Beautiful. https://t.co/JX00OIQOod\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Here we go again... https://t.co/C10qntwqtG\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "He‚Äôs a mentally fragile human. That‚Äôs why he tries to create competition with people who don‚Äôt acknowledge him... H‚Ä¶ https://t.co/M65qLcZp94\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Where did you get this video of my arab brothers when I leave the house semi dressed up https://t.co/fV2oeykoD0\n",
      "Bully Alert!\n",
      "-----------------------------------------\n",
      "And Raymond thinks he‚Äôs going to have me ‚Äúcommitted‚Äù so the government can experiment on My body. üèπ‚öñÔ∏è‚ÄúFor I was hun‚Ä¶ https://t.co/FYtGAzJv6N\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Incredibly disheartened that months after a breakup I still hear my name is being dragged through the mud. While th‚Ä¶ https://t.co/j0DpN6rGav\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "For those that have heard the slanderous rumors, I have never once in my life assaulted a female, ever, and uphold‚Ä¶ https://t.co/jhkS37g1VU\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "I don't know what is true and what is fabricated as mental illness and substance abuse plays a substantial part, bu‚Ä¶ https://t.co/EBkFv64arJ\n",
      "Bully Alert!\n",
      "-----------------------------------------\n",
      "I am loved.\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Thanks @cl_tampabay for the feature! Glad to be partnering with @Tampasdowntown to promote #indiemusic. https://t.co/5atZfjPiGV\n",
      "Nothing to see here\n",
      "-----------------------------------------\n",
      "Tweets from Air Force one. https://t.co/H5XHG88G8m\n",
      "Nothing to see here\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except:\n",
    "        e = sys.exc_info()[1]\n",
    "        #print(\"Error: %s\" % e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
